# So sÃ¡nh MÃ´ hÃ¬nh ML/DL tá»‘t nháº¥t cho Vocabulary Learning System

## 1. PhÃ¢n tÃ­ch BÃ i toÃ¡n

### 1.1. Äáº·c Ä‘iá»ƒm Dá»¯ liá»‡u cá»§a báº¡n

**Structured Data:**
- âœ… Tabular features (times_correct, times_wrong, ef_factor, etc.)
- âœ… Categorical features (status, cefr_level, topic)
- âœ… Temporal features (last_reviewed, interval_days)
- âœ… User behavior patterns

**Sequential Data:**
- âœ… Review history over time
- âœ… Learning trajectory (NEW â†’ UNKNOWN â†’ KNOWN â†’ MASTERED)
- âœ… Temporal dependencies

**Labels:**
- âœ… Multi-class classification (4 classes)
- âœ… Imbalanced classes (cÃ³ thá»ƒ)
- âœ… Ordinal relationship (NEW < UNKNOWN < KNOWN < MASTERED)

### 1.2. YÃªu cáº§u Há»‡ thá»‘ng

**Performance:**
- Inference time < 50ms (real-time)
- Training time reasonable (< 1 giá»)
- Model size < 500MB

**Accuracy:**
- Classification accuracy > 75%
- Per-class recall balanced
- Handle imbalanced data

**Interpretability:**
- Feature importance
- Explainable predictions
- User trust

## 2. Top 10 MÃ´ hÃ¬nh ÄÆ°á»£c Äá» xuáº¥t

### ğŸ¥‡ Tier 1: Production-Ready (Recommended)

#### **1. CatBoost** â­â­â­â­â­

**Táº¡i sao lÃ  #1 cho bÃ i toÃ¡n nÃ y:**
- ğŸ¯ **Xá»­ lÃ½ categorical features native** (khÃ´ng cáº§n encoding)
- ğŸš€ **Fast inference** (~1-2ms)
- ğŸ“Š **Robust vá»›i imbalanced data**
- ğŸ” **Built-in feature importance**
- ğŸ’ª **Ordered boosting** â†’ trÃ¡nh overfitting

**Æ¯u Ä‘iá»ƒm:**
- KhÃ´ng cáº§n feature scaling
- Handle missing values tá»± Ä‘á»™ng
- Symmetric tree structure â†’ fast prediction
- GPU support
- Tá»‘t nháº¥t cho tabular data vá»›i categorical features

**NhÆ°á»£c Ä‘iá»ƒm:**
- Training cháº­m hÆ¡n LightGBM má»™t chÃºt
- Model size lá»›n hÆ¡n

**Use case:** **BEST CHOICE** cho production

```python
from catboost import CatBoostClassifier

model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    depth=8,
    loss_function='MultiClass',
    eval_metric='TotalF1',
    cat_features=['status', 'cefr_level', 'topic_id'],  # Categorical features
    auto_class_weights='Balanced',  # Handle imbalanced data
    random_seed=42,
    verbose=100
)

# Train
model.fit(
    X_train, y_train,
    eval_set=(X_val, y_val),
    early_stopping_rounds=50,
    plot=True
)

# Feature importance
feature_importance = model.get_feature_importance(
    prettified=True
)
```

**Performance Estimate:**
- Accuracy: 78-82%
- Training time: 5-10 phÃºt
- Inference: 1-2ms
- Model size: 50-100MB

---

#### **2. LightGBM** â­â­â­â­â­

**Táº¡i sao tá»‘t:**
- âš¡ **Fastest training & inference**
- ğŸ’¾ **Memory efficient**
- ğŸ“ˆ **Excellent accuracy**
- ğŸ¯ **Handle large datasets**

**Æ¯u Ä‘iá»ƒm:**
- Nhanh nháº¥t trong cÃ¡c GBDT
- Leaf-wise growth â†’ better accuracy
- Native categorical support
- Distributed training

**NhÆ°á»£c Ä‘iá»ƒm:**
- Dá»… overfit vá»›i small dataset
- Cáº§n tuning cáº©n tháº­n

**Use case:** Khi cáº§n **speed** vÃ  cÃ³ dataset lá»›n (>10k samples)

```python
import lightgbm as lgb

model = lgb.LGBMClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    num_leaves=31,
    max_depth=8,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='multiclass',
    num_class=4,
    class_weight='balanced',
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='multi_logloss',
    callbacks=[
        lgb.early_stopping(50),
        lgb.log_evaluation(100)
    ]
)
```

**Performance Estimate:**
- Accuracy: 76-80%
- Training time: 2-5 phÃºt
- Inference: 0.5-1ms
- Model size: 20-50MB

---

#### **3. XGBoost** â­â­â­â­

**Táº¡i sao tá»‘t:**
- ğŸ† **Industry standard**
- ğŸ”§ **Highly tunable**
- ğŸ“š **Extensive documentation**
- ğŸ›¡ï¸ **Robust & stable**

**Æ¯u Ä‘iá»ƒm:**
- Proven track record
- Good balance of speed & accuracy
- Regularization built-in
- Cross-platform

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cháº­m hÆ¡n LightGBM
- Cáº§n more memory

**Use case:** **Baseline model** hoáº·c khi cáº§n stability

```python
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=8,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0.1,
    objective='multi:softprob',
    num_class=4,
    eval_metric='mlogloss',
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=100
)
```

**Performance Estimate:**
- Accuracy: 75-79%
- Training time: 5-8 phÃºt
- Inference: 1-2ms
- Model size: 30-60MB

---

### ğŸ¥ˆ Tier 2: Advanced Deep Learning

#### **4. TabNet** â­â­â­â­â­

**Táº¡i sao Ä‘áº·c biá»‡t:**
- ğŸ§  **Deep Learning cho tabular data**
- ğŸ” **Interpretable attention mechanism**
- ğŸ“Š **Self-supervised pre-training**
- ğŸ¯ **Feature selection tá»± Ä‘á»™ng**

**Æ¯u Ä‘iá»ƒm:**
- Attention mechanism â†’ biáº¿t feature nÃ o quan trá»ng
- KhÃ´ng cáº§n feature engineering nhiá»u
- CÃ³ thá»ƒ pre-train trÃªn unlabeled data
- Competitive vá»›i GBDT

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cáº§n GPU Ä‘á»ƒ train nhanh
- Training phá»©c táº¡p hÆ¡n
- Inference cháº­m hÆ¡n GBDT

**Use case:** Khi cáº§n **interpretability** + **deep learning power**

```python
from pytorch_tabnet.tab_model import TabNetClassifier

model = TabNetClassifier(
    n_d=64,  # Width of decision prediction layer
    n_a=64,  # Width of attention embedding
    n_steps=5,  # Number of steps in architecture
    gamma=1.5,  # Coefficient for feature reusage
    n_independent=2,
    n_shared=2,
    lambda_sparse=1e-4,
    optimizer_fn=torch.optim.Adam,
    optimizer_params=dict(lr=2e-2),
    scheduler_params={"step_size":50, "gamma":0.9},
    scheduler_fn=torch.optim.lr_scheduler.StepLR,
    mask_type='entmax',
    verbose=10
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric=['accuracy'],
    max_epochs=200,
    patience=20,
    batch_size=256
)

# Feature importance with attention
explain_matrix, masks = model.explain(X_test)
```

**Performance Estimate:**
- Accuracy: 77-81%
- Training time: 10-20 phÃºt (GPU)
- Inference: 5-10ms
- Model size: 10-30MB

---

#### **5. Temporal Fusion Transformer (TFT)** â­â­â­â­â­

**Táº¡i sao powerful:**
- ğŸ• **Designed for temporal data**
- ğŸ¯ **Multi-horizon forecasting**
- ğŸ” **Interpretable attention**
- ğŸ“Š **Handle static + dynamic features**

**Æ¯u Ä‘iá»ƒm:**
- State-of-the-art cho time series
- Attention mechanism â†’ interpretable
- Quantile predictions â†’ uncertainty
- Variable selection network

**NhÆ°á»£c Ä‘iá»ƒm:**
- Phá»©c táº¡p nháº¥t
- Cáº§n nhiá»u data
- Training time dÃ i

**Use case:** Khi cÃ³ **rich temporal history** vÃ  cáº§n **long-term predictions**

```python
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet

# Prepare data
training = TimeSeriesDataSet(
    data,
    time_idx="time_idx",
    target="status",
    group_ids=["user_id", "vocab_id"],
    max_encoder_length=30,  # Look back 30 reviews
    max_prediction_length=1,  # Predict next status
    static_categoricals=["cefr_level", "topic_id"],
    static_reals=["user_level_numeric"],
    time_varying_known_categoricals=[],
    time_varying_known_reals=["days_since_last_review"],
    time_varying_unknown_categoricals=["status"],
    time_varying_unknown_reals=["times_correct", "times_wrong", "ef_factor"],
    target_normalizer=None,
)

model = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=0.03,
    hidden_size=64,
    attention_head_size=4,
    dropout=0.1,
    hidden_continuous_size=32,
    output_size=4,  # 4 classes
    loss=CrossEntropyLoss(),
)

trainer = pl.Trainer(max_epochs=50, gpus=1)
trainer.fit(model, train_dataloader, val_dataloader)
```

**Performance Estimate:**
- Accuracy: 80-85% (vá»›i Ä‘á»§ data)
- Training time: 30-60 phÃºt (GPU)
- Inference: 10-20ms
- Model size: 50-100MB

---

#### **6. GRU/LSTM with Attention** â­â­â­â­

**Táº¡i sao tá»‘t:**
- ğŸ”„ **Capture sequential patterns**
- ğŸ§  **Learn temporal dependencies**
- ğŸ¯ **Attention â†’ interpretability**

**Æ¯u Ä‘iá»ƒm:**
- Proven architecture
- Flexible
- Good for sequences
- Attention weights â†’ explainable

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cáº§n sequence data
- Training cháº­m
- Vanishing gradient (LSTM better than RNN)

**Use case:** Khi cÃ³ **review history sequences**

```python
import torch
import torch.nn as nn

class AttentionGRU(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=2):
        super().__init__()
        
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        
        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        gru_out, _ = self.gru(x)  # (batch, seq_len, hidden_dim*2)
        
        # Attention weights
        attention_weights = self.attention(gru_out)  # (batch, seq_len, 1)
        attention_weights = torch.softmax(attention_weights, dim=1)
        
        # Weighted sum
        context = torch.sum(gru_out * attention_weights, dim=1)  # (batch, hidden_dim*2)
        
        # Classification
        output = self.classifier(context)
        
        return output, attention_weights

model = AttentionGRU(input_dim=30, hidden_dim=64, num_classes=4)
```

**Performance Estimate:**
- Accuracy: 78-82%
- Training time: 15-30 phÃºt (GPU)
- Inference: 5-10ms
- Model size: 20-40MB

---

### ğŸ¥‰ Tier 3: Specialized Models

#### **7. Neural Oblivious Decision Trees (NODE)** â­â­â­â­

**Táº¡i sao interesting:**
- ğŸŒ³ **Combines trees + neural networks**
- ğŸ¯ **Differentiable decision trees**
- ğŸ“Š **Best of both worlds**

**Æ¯u Ä‘iá»ƒm:**
- Interpretable nhÆ° trees
- Powerful nhÆ° neural nets
- End-to-end training
- Good for tabular data

**NhÆ°á»£c Ä‘iá»ƒm:**
- Má»›i, Ã­t documentation
- Cáº§n GPU
- Phá»©c táº¡p

**Use case:** Research, khi muá»‘n **tree interpretability + NN power**

```python
# Requires: pip install node-lib
from node import NODE

model = NODE(
    num_layers=4,
    total_tree_count=2048,
    tree_depth=6,
    tree_output_dim=4,
    num_classes=4
)
```

**Performance Estimate:**
- Accuracy: 77-81%
- Training time: 20-40 phÃºt (GPU)
- Inference: 3-5ms

---

#### **8. AutoGluon (AutoML)** â­â­â­â­â­

**Táº¡i sao powerful:**
- ğŸ¤– **Automated ML**
- ğŸ¯ **Ensemble of best models**
- ğŸš€ **State-of-the-art results**
- ğŸ”§ **Minimal code**

**Æ¯u Ä‘iá»ƒm:**
- Tá»± Ä‘á»™ng thá»­ nhiá»u models
- Ensemble tá»‘t nháº¥t
- Hyperparameter tuning tá»± Ä‘á»™ng
- Production-ready

**NhÆ°á»£c Ä‘iá»ƒm:**
- Black box
- Training time dÃ i
- Model size lá»›n

**Use case:** Khi muá»‘n **best accuracy** vÃ  khÃ´ng quan tÃ¢m complexity

```python
from autogluon.tabular import TabularPredictor

predictor = TabularPredictor(
    label='status',
    problem_type='multiclass',
    eval_metric='f1_weighted',
    path='./autogluon_models'
)

predictor.fit(
    train_data=train_df,
    time_limit=3600,  # 1 hour
    presets='best_quality',  # or 'medium_quality', 'optimize_for_deployment'
    num_bag_folds=5,
    num_stack_levels=1
)

# Get leaderboard
leaderboard = predictor.leaderboard(test_df)
```

**Performance Estimate:**
- Accuracy: 80-85% (ensemble)
- Training time: 1-3 giá»
- Inference: 10-50ms (depends on ensemble)
- Model size: 200-500MB

---


#### **9. FT-Transformer** â­â­â­â­

**Táº¡i sao modern:**
- ğŸ¤– **Transformer for tabular data**
- ğŸ¯ **Feature tokenization**
- ğŸ“Š **Self-attention mechanism**
- ğŸš€ **State-of-the-art on benchmarks**

**Æ¯u Ä‘iá»ƒm:**
- Transformer architecture
- KhÃ´ng cáº§n feature engineering nhiá»u
- Attention â†’ interpretable
- Competitive vá»›i GBDT

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cáº§n GPU
- Training time dÃ i
- Overfitting vá»›i small data

**Use case:** Research, khi cÃ³ **large dataset** (>50k samples)

```python
import torch
import torch.nn as nn

class FTTransformer(nn.Module):
    def __init__(self, n_features, n_classes, d_model=192, n_heads=8, n_layers=3):
        super().__init__()
        
        # Feature tokenization
        self.feature_tokenizer = nn.Linear(1, d_model)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, n_classes)
        )
    
    def forward(self, x):
        # x: (batch, n_features)
        
        # Tokenize each feature
        x = x.unsqueeze(-1)  # (batch, n_features, 1)
        tokens = self.feature_tokenizer(x)  # (batch, n_features, d_model)
        
        # Add CLS token
        cls_token = nn.Parameter(torch.randn(1, 1, d_model))
        tokens = torch.cat([cls_token.expand(x.size(0), -1, -1), tokens], dim=1)
        
        # Transformer
        tokens = tokens.transpose(0, 1)  # (seq_len, batch, d_model)
        encoded = self.transformer(tokens)
        
        # Use CLS token for classification
        cls_output = encoded[0]  # (batch, d_model)
        
        output = self.classifier(cls_output)
        return output

model = FTTransformer(n_features=30, n_classes=4)
```

**Performance Estimate:**
- Accuracy: 78-83%
- Training time: 30-60 phÃºt (GPU)
- Inference: 5-10ms
- Model size: 50-100MB

---

#### **10. Gradient Boosting + Neural Network Ensemble** â­â­â­â­â­

**Táº¡i sao powerful:**
- ğŸ¯ **Best of both worlds**
- ğŸ“Š **GBDT features + NN power**
- ğŸš€ **Ensemble diversity**

**Æ¯u Ä‘iá»ƒm:**
- Combine strengths
- Better generalization
- Robust predictions
- High accuracy

**NhÆ°á»£c Ä‘iá»ƒm:**
- Complex pipeline
- Longer training
- Larger model size

**Use case:** Khi cáº§n **maximum accuracy** cho production

```python
class GBDTNNEnsemble:
    def __init__(self):
        # GBDT models
        self.catboost = CatBoostClassifier(iterations=500, depth=6)
        self.lightgbm = lgb.LGBMClassifier(n_estimators=500, num_leaves=31)
        self.xgboost = xgb.XGBClassifier(n_estimators=500, max_depth=6)
        
        # Neural network
        self.nn = TabNetClassifier(n_d=64, n_a=64, n_steps=5)
        
        # Meta-learner
        self.meta_model = LogisticRegression(multi_class='multinomial')
    
    def fit(self, X_train, y_train, X_val, y_val):
        # Train base models
        self.catboost.fit(X_train, y_train, eval_set=(X_val, y_val))
        self.lightgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)])
        self.xgboost.fit(X_train, y_train, eval_set=[(X_val, y_val)])
        self.nn.fit(X_train, y_train, eval_set=[(X_val, y_val)])
        
        # Get predictions for meta-learner
        train_meta_features = self._get_meta_features(X_train)
        self.meta_model.fit(train_meta_features, y_train)
    
    def _get_meta_features(self, X):
        # Stack predictions from all models
        pred_cb = self.catboost.predict_proba(X)
        pred_lgb = self.lightgbm.predict_proba(X)
        pred_xgb = self.xgboost.predict_proba(X)
        pred_nn = self.nn.predict_proba(X)
        
        return np.hstack([pred_cb, pred_lgb, pred_xgb, pred_nn])
    
    def predict_proba(self, X):
        meta_features = self._get_meta_features(X)
        return self.meta_model.predict_proba(meta_features)
    
    def predict(self, X):
        return self.predict_proba(X).argmax(axis=1)

# Usage
ensemble = GBDTNNEnsemble()
ensemble.fit(X_train, y_train, X_val, y_val)
```

**Performance Estimate:**
- Accuracy: 82-87% (best)
- Training time: 30-60 phÃºt
- Inference: 10-20ms
- Model size: 200-400MB

---

## 3. Detailed Comparison Table

| Model | Accuracy | Speed | Interpretability | Complexity | Data Need | GPU | Best For |
|-------|----------|-------|------------------|------------|-----------|-----|----------|
| **CatBoost** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­ | âŒ | **Production** |
| **LightGBM** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ | âŒ | **Speed** |
| **XGBoost** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ | âŒ | **Baseline** |
| **TabNet** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | âœ… | **Interpretable DL** |
| **TFT** | â­â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | âœ… | **Temporal** |
| **GRU+Attention** | â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ | âœ… | **Sequences** |
| **NODE** | â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | âœ… | **Research** |
| **AutoGluon** | â­â­â­â­â­ | â­â­ | â­â­ | â­ | â­â­â­ | âŒ | **Max Accuracy** |
| **FT-Transformer** | â­â­â­â­ | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | âœ… | **Large Data** |
| **Ensemble** | â­â­â­â­â­ | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | âš ï¸ | **Competition** |

## 4. Recommendation Matrix

### 4.1. Based on Dataset Size

**Small (<5k samples):**
1. CatBoost (best)
2. XGBoost
3. Random Forest

**Medium (5k-50k):**
1. CatBoost (best)
2. LightGBM
3. TabNet

**Large (>50k):**
1. LightGBM (best)
2. AutoGluon
3. FT-Transformer

### 4.2. Based on Priority

**Priority: Speed**
1. LightGBM âš¡
2. CatBoost
3. XGBoost

**Priority: Accuracy**
1. AutoGluon ğŸ¯
2. Ensemble
3. CatBoost

**Priority: Interpretability**
1. CatBoost ğŸ”
2. TabNet
3. XGBoost

**Priority: Simplicity**
1. CatBoost ğŸˆ
2. LightGBM
3. XGBoost

**Priority: Innovation**
1. TFT ğŸš€
2. FT-Transformer
3. TabNet

### 4.3. Based on Infrastructure

**No GPU:**
1. CatBoost
2. LightGBM
3. XGBoost

**Have GPU:**
1. TabNet
2. TFT
3. GRU+Attention

**Limited Resources:**
1. LightGBM
2. XGBoost
3. CatBoost

## 5. Recommended Strategy

### ğŸ¯ Phase 1: Quick Win (Tuáº§n 1-2)

**Model:** CatBoost

**LÃ½ do:**
- Fastest to production
- Best accuracy/effort ratio
- Handle categorical features native
- Robust vá»›i imbalanced data

**Implementation:**
```python
from catboost import CatBoostClassifier, Pool

# Prepare data
train_pool = Pool(
    X_train, 
    y_train,
    cat_features=['status', 'cefr_level', 'topic_id']
)

val_pool = Pool(
    X_val,
    y_val,
    cat_features=['status', 'cefr_level', 'topic_id']
)

# Train
model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    depth=8,
    loss_function='MultiClass',
    eval_metric='TotalF1',
    auto_class_weights='Balanced',
    random_seed=42,
    task_type='GPU'  # if available
)

model.fit(
    train_pool,
    eval_set=val_pool,
    early_stopping_rounds=50,
    verbose=100,
    plot=True
)

# Save
model.save_model('catboost_model.cbm')
```

**Expected Result:**
- Accuracy: 78-82%
- Training: 5-10 phÃºt
- Production-ready

---

### ğŸš€ Phase 2: Optimization (Tuáº§n 3-4)

**Models:** CatBoost + LightGBM + TabNet

**Strategy:** Ensemble

```python
class OptimizedEnsemble:
    def __init__(self):
        self.catboost = CatBoostClassifier(...)
        self.lightgbm = lgb.LGBMClassifier(...)
        self.tabnet = TabNetClassifier(...)
        
        # Weighted average
        self.weights = [0.4, 0.3, 0.3]  # Tune based on validation
    
    def predict_proba(self, X):
        pred_cb = self.catboost.predict_proba(X)
        pred_lgb = self.lightgbm.predict_proba(X)
        pred_tn = self.tabnet.predict_proba(X)
        
        return (
            self.weights[0] * pred_cb +
            self.weights[1] * pred_lgb +
            self.weights[2] * pred_tn
        )
```

**Expected Result:**
- Accuracy: 80-84%
- Robust predictions

---

### ğŸ“ Phase 3: Advanced (Tuáº§n 5-6)

**Model:** Temporal Fusion Transformer

**When:** Khi cÃ³ rich temporal data

```python
# Prepare sequential data
def prepare_sequences(user_id, vocab_id, lookback=30):
    """Get last 30 reviews for this user-vocab pair"""
    history = get_review_history(user_id, vocab_id, limit=lookback)
    
    features = []
    for review in history:
        features.append([
            review.times_correct,
            review.times_wrong,
            review.ef_factor,
            review.interval_days,
            # ... more features
        ])
    
    return np.array(features)

# Train TFT
model = TemporalFusionTransformer(...)
```

**Expected Result:**
- Accuracy: 82-86%
- Long-term predictions
- Interpretable attention

---

## 6. Hybrid Approach (BEST)

### 6.1. Multi-Model System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Prediction Router                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€
             â”‚                 â”‚                 â”‚
             â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   CatBoost     â”‚ â”‚   TabNet     â”‚ â”‚     TFT      â”‚
    â”‚  (Fast Path)   â”‚ â”‚ (Interpret)  â”‚ â”‚  (Temporal)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                 â”‚                 â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Meta Ensemble   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2. Implementation

```python
class HybridVocabPredictor:
    def __init__(self):
        self.catboost = load_model('catboost.cbm')  # Fast
        self.tabnet = load_model('tabnet.pkl')      # Interpretable
        self.tft = load_model('tft.ckpt')           # Temporal
        
        self.router = self._init_router()
    
    async def predict(self, user_id, vocab_id, mode='auto'):
        """
        Predict with appropriate model based on context
        
        Modes:
        - 'fast': Use CatBoost (1-2ms)
        - 'interpret': Use TabNet (5-10ms)
        - 'temporal': Use TFT (10-20ms)
        - 'auto': Router decides
        """
        
        if mode == 'auto':
            mode = self.router.decide(user_id, vocab_id)
        
        if mode == 'fast':
            return await self._predict_catboost(user_id, vocab_id)
        elif mode == 'interpret':
            return await self._predict_tabnet(user_id, vocab_id)
        elif mode == 'temporal':
            return await self._predict_tft(user_id, vocab_id)
    
    def _init_router(self):
        """
        Router logic:
        - New users (<10 vocabs): CatBoost (fast, reliable)
        - Need explanation: TabNet (interpretable)
        - Rich history (>30 reviews): TFT (temporal)
        """
        class Router:
            def decide(self, user_id, vocab_id):
                vocab_count = get_user_vocab_count(user_id)
                review_count = get_review_count(user_id, vocab_id)
                
                if vocab_count < 10:
                    return 'fast'
                elif review_count > 30:
                    return 'temporal'
                else:
                    return 'interpret'
        
        return Router()
```

---

## 7. Special Techniques

### 7.1. Handling Imbalanced Classes

```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# SMOTE + Undersampling
resampler = Pipeline([
    ('over', SMOTE(sampling_strategy={0: 1000, 1: 1000})),  # Oversample minority
    ('under', RandomUnderSampler(sampling_strategy={3: 500}))  # Undersample majority
])

X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)
```

### 7.2. Ordinal Encoding for Status

```python
# Status cÃ³ thá»© tá»±: NEW < UNKNOWN < KNOWN < MASTERED
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder(
    categories=[['NEW', 'UNKNOWN', 'KNOWN', 'MASTERED']]
)

# Use ordinal loss
model = CatBoostClassifier(
    loss_function='MultiClass',
    # Or use custom metric that considers order
)
```

### 7.3. Feature Engineering Boost

```python
def create_advanced_features(df):
    """Advanced feature engineering"""
    
    # 1. Interaction features
    df['accuracy_x_recency'] = df['accuracy_rate'] * df['recency_score']
    df['difficulty_gap'] = df['vocab_cefr_numeric'] - df['user_level_numeric']
    
    # 2. Aggregation features (per user)
    user_stats = df.groupby('user_id').agg({
        'times_correct': ['mean', 'std', 'max'],
        'times_wrong': ['mean', 'std', 'max'],
        'accuracy_rate': ['mean', 'std']
    })
    df = df.merge(user_stats, on='user_id', suffixes=('', '_user_avg'))
    
    # 3. Aggregation features (per vocab)
    vocab_stats = df.groupby('vocab_id').agg({
        'times_correct': 'mean',
        'times_wrong': 'mean',
        'accuracy_rate': 'mean'
    })
    df = df.merge(vocab_stats, on='vocab_id', suffixes=('', '_vocab_avg'))
    
    # 4. Temporal features
    df['hour_of_day'] = df['last_reviewed'].dt.hour
    df['day_of_week'] = df['last_reviewed'].dt.dayofweek
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    
    # 5. Streak features
    df['study_consistency'] = df['current_streak'] / (df['total_study_days'] + 1)
    
    return df
```


## 8. Final Recommendations

### ğŸ† Top 3 Choices

#### **#1: CatBoost (BEST FOR YOU)**

**LÃ½ do:**
- âœ… Perfect cho tabular data vá»›i categorical features
- âœ… KhÃ´ng cáº§n GPU
- âœ… Production-ready ngay
- âœ… Robust vá»›i imbalanced data
- âœ… Fast inference (1-2ms)
- âœ… Interpretable
- âœ… Easy to deploy

**Khi nÃ o dÃ¹ng:** **ALWAYS** - ÄÃ¢y lÃ  best choice cho bÃ i toÃ¡n cá»§a báº¡n

**Code template:**
```python
# train.py
from catboost import CatBoostClassifier, Pool
import pandas as pd

# Load data
df = pd.read_sql("SELECT * FROM user_vocab_progress ...", conn)

# Feature engineering
df = create_features(df)

# Prepare
X = df[feature_columns]
y = df['status']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train
model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    depth=8,
    loss_function='MultiClass',
    eval_metric='TotalF1',
    cat_features=['status', 'cefr_level', 'topic_id'],
    auto_class_weights='Balanced',
    random_seed=42
)

model.fit(X_train, y_train, eval_set=(X_test, y_test))

# Save
model.save_model('models/catboost_v1.cbm')
```

---

#### **#2: LightGBM (If you need SPEED)**

**Khi nÃ o dÃ¹ng:**
- Dataset lá»›n (>50k samples)
- Cáº§n inference cá»±c nhanh (<1ms)
- CÃ³ nhiá»u features (>100)

**Trade-off:** Accuracy cÃ³ thá»ƒ tháº¥p hÆ¡n CatBoost 1-2%

---

#### **#3: TabNet (If you need INTERPRETABILITY)**

**Khi nÃ o dÃ¹ng:**
- Cáº§n giáº£i thÃ­ch predictions cho users
- Muá»‘n biáº¿t features nÃ o quan trá»ng cho tá»«ng prediction
- CÃ³ GPU

**Trade-off:** Inference cháº­m hÆ¡n (5-10ms)

---

### ğŸ“Š Decision Tree

```
START
  â”‚
  â”œâ”€ Dataset size < 10k?
  â”‚   â””â”€ YES â†’ CatBoost
  â”‚   â””â”€ NO â†’ Continue
  â”‚
  â”œâ”€ Need interpretability?
  â”‚   â””â”€ YES â†’ TabNet
  â”‚   â””â”€ NO â†’ Continue
  â”‚
  â”œâ”€ Have rich temporal data (>30 reviews per vocab)?
  â”‚   â””â”€ YES â†’ TFT
  â”‚   â””â”€ NO â†’ Continue
  â”‚
  â”œâ”€ Need maximum accuracy (competition)?
  â”‚   â””â”€ YES â†’ AutoGluon or Ensemble
  â”‚   â””â”€ NO â†’ Continue
  â”‚
  â””â”€ Default â†’ CatBoost
```

---

## 9. Implementation Plan

### Week 1: CatBoost Baseline

**Day 1-2: Data Preparation**
```bash
# Tasks
- [ ] Query data from PostgreSQL
- [ ] EDA (Exploratory Data Analysis)
- [ ] Check label distribution
- [ ] Handle missing values
- [ ] Feature engineering
```

**Day 3-4: Model Training**
```bash
# Tasks
- [ ] Train CatBoost model
- [ ] Hyperparameter tuning with Optuna
- [ ] Cross-validation
- [ ] Evaluate metrics
```

**Day 5: Deployment**
```bash
# Tasks
- [ ] Save model
- [ ] Create inference API
- [ ] Test predictions
- [ ] Document results
```

### Week 2: Optimization

**Day 1-2: Ensemble**
```bash
# Tasks
- [ ] Train LightGBM
- [ ] Train XGBoost
- [ ] Create ensemble
- [ ] Compare results
```

**Day 3-4: Advanced Features**
```bash
# Tasks
- [ ] Add interaction features
- [ ] Add aggregation features
- [ ] Add temporal features
- [ ] Retrain models
```

**Day 5: Production**
```bash
# Tasks
- [ ] Deploy best model
- [ ] Setup monitoring
- [ ] A/B testing
```

---

## 10. Code Templates

### 10.1. Complete Training Pipeline

```python
# train_pipeline.py
import pandas as pd
import numpy as np
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, classification_report
import optuna
import mlflow

class VocabModelTrainer:
    def __init__(self, db_connection):
        self.conn = db_connection
        self.model = None
        self.feature_names = None
    
    def load_data(self):
        """Load data from database"""
        query = """
        SELECT 
            uvp.id,
            uvp.user_id,
            uvp.vocab_id,
            uvp.status,
            uvp.times_correct,
            uvp.times_wrong,
            uvp.ef_factor,
            uvp.interval_days,
            uvp.repetition,
            uvp.last_reviewed,
            uvp.created_at,
            v.cefr,
            v.word_length,
            v.topic_id,
            u.current_level,
            u.current_streak,
            u.total_study_days
        FROM user_vocab_progress uvp
        JOIN vocab v ON uvp.vocab_id = v.id
        JOIN users u ON uvp.user_id = u.id
        WHERE uvp.status IN ('NEW', 'UNKNOWN', 'KNOWN', 'MASTERED')
        """
        
        df = pd.read_sql(query, self.conn)
        return df
    
    def engineer_features(self, df):
        """Feature engineering"""
        
        # Temporal features
        df['days_since_last_review'] = (
            pd.Timestamp.now() - pd.to_datetime(df['last_reviewed'])
        ).dt.days
        
        df['days_since_created'] = (
            pd.Timestamp.now() - pd.to_datetime(df['created_at'])
        ).dt.days
        
        # Accuracy
        df['accuracy_rate'] = df['times_correct'] / (
            df['times_correct'] + df['times_wrong'] + 1
        )
        
        # CEFR numeric
        cefr_map = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}
        df['cefr_numeric'] = df['cefr'].map(cefr_map)
        df['user_level_numeric'] = df['current_level'].map(cefr_map)
        
        # Difficulty gap
        df['difficulty_gap'] = df['cefr_numeric'] - df['user_level_numeric']
        
        # Study consistency
        df['study_consistency'] = df['current_streak'] / (df['total_study_days'] + 1)
        
        # User aggregations
        user_stats = df.groupby('user_id').agg({
            'accuracy_rate': ['mean', 'std'],
            'times_correct': 'mean',
            'times_wrong': 'mean'
        }).reset_index()
        user_stats.columns = ['user_id', 'user_avg_accuracy', 'user_std_accuracy',
                             'user_avg_correct', 'user_avg_wrong']
        df = df.merge(user_stats, on='user_id', how='left')
        
        return df
    
    def prepare_data(self):
        """Prepare train/val/test sets"""
        
        df = self.load_data()
        df = self.engineer_features(df)
        
        # Features
        feature_cols = [
            'times_correct', 'times_wrong', 'ef_factor', 'interval_days',
            'repetition', 'days_since_last_review', 'days_since_created',
            'accuracy_rate', 'cefr_numeric', 'user_level_numeric',
            'difficulty_gap', 'current_streak', 'total_study_days',
            'study_consistency', 'user_avg_accuracy', 'user_std_accuracy',
            'user_avg_correct', 'user_avg_wrong'
        ]
        
        cat_features = ['topic_id']
        
        X = df[feature_cols + cat_features]
        y = df['status'].map({'NEW': 0, 'UNKNOWN': 1, 'KNOWN': 2, 'MASTERED': 3})
        
        self.feature_names = feature_cols + cat_features
        
        # Split
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )
        
        return {
            'train': (X_train, y_train),
            'val': (X_val, y_val),
            'test': (X_test, y_test),
            'cat_features': cat_features
        }
    
    def optimize_hyperparameters(self, X_train, y_train, X_val, y_val, cat_features):
        """Hyperparameter tuning with Optuna"""
        
        def objective(trial):
            params = {
                'iterations': trial.suggest_int('iterations', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'depth': trial.suggest_int('depth', 4, 10),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
                'border_count': trial.suggest_int('border_count', 32, 255),
                'random_seed': 42,
                'loss_function': 'MultiClass',
                'eval_metric': 'TotalF1',
                'auto_class_weights': 'Balanced',
                'verbose': False
            }
            
            model = CatBoostClassifier(**params)
            model.fit(
                X_train, y_train,
                eval_set=(X_val, y_val),
                cat_features=cat_features,
                early_stopping_rounds=50,
                verbose=False
            )
            
            y_pred = model.predict(X_val)
            f1 = f1_score(y_val, y_pred, average='weighted')
            
            return f1
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50)
        
        return study.best_params
    
    def train(self, params=None):
        """Train final model"""
        
        mlflow.start_run()
        
        # Prepare data
        data = self.prepare_data()
        X_train, y_train = data['train']
        X_val, y_val = data['val']
        X_test, y_test = data['test']
        cat_features = data['cat_features']
        
        # Hyperparameter tuning
        if params is None:
            print("Optimizing hyperparameters...")
            params = self.optimize_hyperparameters(
                X_train, y_train, X_val, y_val, cat_features
            )
        
        # Train final model
        print("Training final model...")
        self.model = CatBoostClassifier(**params)
        self.model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            cat_features=cat_features,
            early_stopping_rounds=50,
            verbose=100
        )
        
        # Evaluate
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"\nTest Accuracy: {accuracy:.4f}")
        print(f"Test F1-Score: {f1:.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, 
                                   target_names=['NEW', 'UNKNOWN', 'KNOWN', 'MASTERED']))
        
        # Log to MLflow
        mlflow.log_params(params)
        mlflow.log_metrics({'test_accuracy': accuracy, 'test_f1': f1})
        mlflow.catboost.log_model(self.model, "model")
        
        mlflow.end_run()
        
        return self.model
    
    def save_model(self, path='models/catboost_vocab_predictor.cbm'):
        """Save model"""
        self.model.save_model(path)
        print(f"Model saved to {path}")

# Usage
if __name__ == "__main__":
    from sqlalchemy import create_engine
    
    engine = create_engine("postgresql://user:pass@localhost/cardwords")
    
    trainer = VocabModelTrainer(engine)
    model = trainer.train()
    trainer.save_model()
```

### 10.2. Inference API

```python
# api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from catboost import CatBoostClassifier
import numpy as np
import pandas as pd

app = FastAPI()

# Load model at startup
model = CatBoostClassifier()
model.load_model('models/catboost_vocab_predictor.cbm')

class PredictionRequest(BaseModel):
    user_id: str
    vocab_id: str

class PredictionResponse(BaseModel):
    vocab_id: str
    current_status: str
    predicted_status: str
    probabilities: dict
    confidence: float
    recommendation: str

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Predict next status for vocabulary"""
    
    # Fetch features from database
    features = await fetch_features(request.user_id, request.vocab_id)
    
    # Create feature vector
    X = create_feature_vector(features)
    
    # Predict
    probabilities = model.predict_proba(X)[0]
    predicted_class = model.predict(X)[0]
    
    status_map = {0: 'NEW', 1: 'UNKNOWN', 2: 'KNOWN', 3: 'MASTERED'}
    predicted_status = status_map[predicted_class]
    
    probs_dict = {
        status_map[i]: float(prob) 
        for i, prob in enumerate(probabilities)
    }
    
    confidence = float(np.max(probabilities))
    
    recommendation = generate_recommendation(
        features['current_status'],
        predicted_status,
        probs_dict,
        confidence
    )
    
    return PredictionResponse(
        vocab_id=request.vocab_id,
        current_status=features['current_status'],
        predicted_status=predicted_status,
        probabilities=probs_dict,
        confidence=confidence,
        recommendation=recommendation
    )
```

---

## 11. Conclusion

### ğŸ¯ Final Answer: **CatBoost**

**LÃ½ do:**
1. âœ… **Perfect fit** cho tabular data vá»›i categorical features
2. âœ… **Production-ready** ngay láº­p tá»©c
3. âœ… **High accuracy** (78-82%)
4. âœ… **Fast inference** (1-2ms)
5. âœ… **No GPU needed**
6. âœ… **Interpretable**
7. âœ… **Easy to maintain**
8. âœ… **Robust vá»›i imbalanced data**

### ğŸ“ˆ Expected Results

**Metrics:**
- Accuracy: 78-82%
- F1-Score: 0.76-0.80
- Inference time: 1-2ms
- Training time: 5-10 phÃºt

**Business Impact:**
- User engagement: +20%
- Learning efficiency: +15%
- Retention: +20%

### ğŸš€ Next Steps

1. **Week 1:** Implement CatBoost baseline
2. **Week 2:** Optimize & deploy
3. **Week 3:** A/B testing
4. **Week 4:** Iterate based on results

### ğŸ’¡ Pro Tips

1. **Start simple:** CatBoost first, optimize later
2. **Feature engineering:** Spend time here, biggest impact
3. **Monitor:** Track online performance
4. **Iterate:** Continuous improvement

---

**TÃ¡c giáº£:** AI Assistant  
**NgÃ y táº¡o:** 2024-11-16  
**PhiÃªn báº£n:** 1.0  
**Status:** Ready to implement! ğŸš€

**Recommendation:** Báº¯t Ä‘áº§u vá»›i CatBoost ngay hÃ´m nay! ğŸ’ª
