# CÃ¡ch Tiáº¿p Cáº­n Tá»‘t Nháº¥t: Temporal Sequence Learning

## Váº¥n Äá» Vá»›i CÃ¡ch Hiá»‡n Táº¡i

### CÃ¡ch 1 & 2: Snapshot-Based Learning âŒ

```
Record táº¡i thá»i Ä‘iá»ƒm T:
- times_correct: 3
- times_wrong: 7
- accuracy: 0.3
â†’ Predict: forgot = 1
```

**Váº¥n Ä‘á»:**
- âŒ Chá»‰ nhÃ¬n 1 snapshot táº¡i 1 thá»i Ä‘iá»ƒm
- âŒ KhÃ´ng biáº¿t **xu hÆ°á»›ng** (trend): Ä‘ang tiáº¿n bá»™ hay thoÃ¡i bá»™?
- âŒ KhÃ´ng biáº¿t **pattern**: há»c Ä‘á»u hay há»c dá»“n?
- âŒ Máº¥t thÃ´ng tin vá» **learning curve**

### VÃ­ Dá»¥ Minh Há»a

**User A:**
```
Day 1: 1 correct, 4 wrong (20% accuracy)
Day 2: 2 correct, 3 wrong (40% accuracy)
Day 3: 3 correct, 2 wrong (60% accuracy) â† Äang tiáº¿n bá»™! ğŸ“ˆ
```

**User B:**
```
Day 1: 3 correct, 2 wrong (60% accuracy)
Day 2: 2 correct, 3 wrong (40% accuracy)
Day 3: 1 correct, 4 wrong (20% accuracy) â† Äang thoÃ¡i bá»™! ğŸ“‰
```

**Cáº£ 2 user Ä‘á»u cÃ³ tá»•ng: 6 correct, 9 wrong (40% accuracy)**
â†’ NhÆ°ng User A Ä‘ang há»c tá»‘t, User B Ä‘ang quÃªn!

**Snapshot-based khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c!** âŒ

---

## Giáº£i PhÃ¡p: Temporal Sequence Learning âœ…

### Ã TÆ°á»Ÿng ChÃ­nh

**Há»c tá»« CHUá»–I cÃ¡c attempts theo thá»i gian:**

```
Vocab "abandon":
Attempt 1 (Day 1): Wrong â†’ forgot = 1
Attempt 2 (Day 3): Wrong â†’ forgot = 1
Attempt 3 (Day 5): Correct â†’ forgot = 0
Attempt 4 (Day 8): Correct â†’ forgot = 0
Attempt 5 (Day 12): ? â†’ Predict nÃ y!
```

**Features bao gá»“m:**
- Current state (nhÆ° cÅ©)
- **Previous attempts** (3-5 láº§n gáº§n nháº¥t)
- **Time intervals** giá»¯a cÃ¡c attempts
- **Trend** (Ä‘ang tiáº¿n bá»™ hay thoÃ¡i bá»™)
- **Consistency** (á»•n Ä‘á»‹nh hay tháº¥t thÆ°á»ng)


## Implementation: Query Temporal Data

### SQL Query: Láº¥y Sequence Data

```sql
WITH vocab_attempts AS (
    SELECT 
        gsd.vocab_id,
        gs.user_id,
        gsd.is_correct,
        gsd.time_taken,
        gsd.created_at,
        ROW_NUMBER() OVER (
            PARTITION BY gs.user_id, gsd.vocab_id 
            ORDER BY gsd.created_at DESC
        ) as attempt_rank
    FROM game_session_details gsd
    JOIN game_sessions gs ON gsd.session_id = gs.id
    WHERE gs.finished_at IS NOT NULL
),
recent_attempts AS (
    SELECT 
        user_id,
        vocab_id,
        MAX(CASE WHEN attempt_rank = 1 THEN is_correct END) as last_attempt_1,
        MAX(CASE WHEN attempt_rank = 2 THEN is_correct END) as last_attempt_2,
        MAX(CASE WHEN attempt_rank = 3 THEN is_correct END) as last_attempt_3,
        MAX(CASE WHEN attempt_rank = 4 THEN is_correct END) as last_attempt_4,
        MAX(CASE WHEN attempt_rank = 5 THEN is_correct END) as last_attempt_5,
        
        -- Time intervals
        MAX(CASE WHEN attempt_rank = 1 THEN created_at END) as time_1,
        MAX(CASE WHEN attempt_rank = 2 THEN created_at END) as time_2,
        MAX(CASE WHEN attempt_rank = 3 THEN created_at END) as time_3,
        
        -- Calculate intervals
        EXTRACT(DAY FROM 
            MAX(CASE WHEN attempt_rank = 1 THEN created_at END) - 
            MAX(CASE WHEN attempt_rank = 2 THEN created_at END)
        ) as interval_1_2,
        EXTRACT(DAY FROM 
            MAX(CASE WHEN attempt_rank = 2 THEN created_at END) - 
            MAX(CASE WHEN attempt_rank = 3 THEN created_at END)
        ) as interval_2_3,
        
        -- Trend calculation
        (CASE WHEN MAX(CASE WHEN attempt_rank = 1 THEN is_correct END) THEN 1 ELSE 0 END +
         CASE WHEN MAX(CASE WHEN attempt_rank = 2 THEN is_correct END) THEN 1 ELSE 0 END +
         CASE WHEN MAX(CASE WHEN attempt_rank = 3 THEN is_correct END) THEN 1 ELSE 0 END) as recent_3_correct,
        
        COUNT(*) as total_attempts
    FROM vocab_attempts
    WHERE attempt_rank <= 5
    GROUP BY user_id, vocab_id
),
user_stats AS (
    SELECT 
        u.id as user_id,
        COUNT(DISTINCT uvp.vocab_id) as total_vocabs,
        AVG(CASE 
            WHEN uvp.times_correct + uvp.times_wrong > 0 
            THEN uvp.times_correct::numeric / (uvp.times_correct + uvp.times_wrong)
            ELSE 0 
        END) as user_accuracy,
        u.current_streak,
        u.total_study_days
    FROM users u
    LEFT JOIN user_vocab_progress uvp ON u.id = uvp.user_id
    GROUP BY u.id
)
SELECT 
    uvp.user_id,
    uvp.vocab_id,
    v.word,
    
    -- User features
    us.total_vocabs as user_total_vocabs,
    us.user_accuracy,
    us.current_streak,
    us.total_study_days,
    
    -- Vocab features
    CASE v.cefr 
        WHEN 'A1' THEN 1 WHEN 'A2' THEN 2 
        WHEN 'B1' THEN 3 WHEN 'B2' THEN 4 
        WHEN 'C1' THEN 5 WHEN 'C2' THEN 6 
        ELSE 3 
    END as vocab_difficulty,
    LENGTH(v.word) as vocab_length,
    
    -- Current progress features
    uvp.times_correct,
    uvp.times_wrong,
    CASE 
        WHEN uvp.times_correct + uvp.times_wrong > 0 
        THEN uvp.times_correct::numeric / (uvp.times_correct + uvp.times_wrong)
        ELSE 0 
    END as accuracy_rate,
    uvp.repetition,
    uvp.ef_factor,
    uvp.interval_days,
    COALESCE(EXTRACT(DAY FROM (CURRENT_DATE - uvp.last_reviewed)), 999) as days_since_last_review,
    
    -- â­ TEMPORAL FEATURES (NEW!)
    CASE WHEN ra.last_attempt_1 THEN 1 ELSE 0 END as last_attempt_1,
    CASE WHEN ra.last_attempt_2 THEN 1 ELSE 0 END as last_attempt_2,
    CASE WHEN ra.last_attempt_3 THEN 1 ELSE 0 END as last_attempt_3,
    CASE WHEN ra.last_attempt_4 THEN 1 ELSE 0 END as last_attempt_4,
    CASE WHEN ra.last_attempt_5 THEN 1 ELSE 0 END as last_attempt_5,
    
    COALESCE(ra.interval_1_2, 0) as interval_1_2,
    COALESCE(ra.interval_2_3, 0) as interval_2_3,
    
    COALESCE(ra.recent_3_correct, 0) as recent_3_correct,
    COALESCE(ra.total_attempts, 0) as total_attempts,
    
    -- Trend features
    CASE 
        WHEN ra.recent_3_correct >= 2 THEN 1  -- Improving
        WHEN ra.recent_3_correct <= 1 THEN -1 -- Declining
        ELSE 0                                 -- Stable
    END as learning_trend,
    
    -- Consistency (variance in intervals)
    CASE 
        WHEN ABS(COALESCE(ra.interval_1_2, 0) - COALESCE(ra.interval_2_3, 0)) <= 2 THEN 1
        ELSE 0
    END as is_consistent,
    
    -- Target: Next attempt result
    -- Láº¥y tá»« attempt tiáº¿p theo (náº¿u cÃ³)
    (SELECT is_correct 
     FROM vocab_attempts va 
     WHERE va.user_id = uvp.user_id 
       AND va.vocab_id = uvp.vocab_id 
       AND va.created_at > uvp.updated_at
     ORDER BY va.created_at ASC 
     LIMIT 1
    ) as next_attempt_result,
    
    CASE 
        WHEN (SELECT is_correct 
              FROM vocab_attempts va 
              WHERE va.user_id = uvp.user_id 
                AND va.vocab_id = uvp.vocab_id 
                AND va.created_at > uvp.updated_at
              ORDER BY va.created_at ASC 
              LIMIT 1) = false THEN 1
        WHEN (SELECT is_correct 
              FROM vocab_attempts va 
              WHERE va.user_id = uvp.user_id 
                AND va.vocab_id = uvp.vocab_id 
                AND va.created_at > uvp.updated_at
              ORDER BY va.created_at ASC 
              LIMIT 1) = true THEN 0
        ELSE NULL
    END as forgot

FROM user_vocab_progress uvp
JOIN vocabs v ON uvp.vocab_id = v.id
JOIN user_stats us ON uvp.user_id = us.user_id
LEFT JOIN recent_attempts ra ON uvp.user_id = ra.user_id AND uvp.vocab_id = ra.vocab_id
WHERE uvp.times_correct + uvp.times_wrong > 0
    AND uvp.status IN ('KNOWN', 'UNKNOWN', 'MASTERED')
ORDER BY uvp.updated_at DESC;
```

**Key Features Má»›i:**
- `last_attempt_1` Ä‘áº¿n `last_attempt_5`: Káº¿t quáº£ 5 láº§n gáº§n nháº¥t
- `interval_1_2`, `interval_2_3`: Khoáº£ng cÃ¡ch thá»i gian giá»¯a cÃ¡c attempts
- `recent_3_correct`: Sá»‘ láº§n Ä‘Ãºng trong 3 láº§n gáº§n nháº¥t
- `learning_trend`: Xu hÆ°á»›ng (1=tiáº¿n bá»™, -1=thoÃ¡i bá»™, 0=á»•n Ä‘á»‹nh)
- `is_consistent`: CÃ³ há»c Ä‘á»u khÃ´ng


## Python Training vá»›i Temporal Features

```python
# train_temporal_model.py
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report
import joblib

class TemporalModelTrainer:
    def __init__(self):
        self.feature_columns = [
            # User features
            'user_total_vocabs', 'user_accuracy', 'current_streak', 'total_study_days',
            
            # Vocab features
            'vocab_difficulty', 'vocab_length',
            
            # Current progress
            'times_correct', 'times_wrong', 'accuracy_rate',
            'repetition', 'ef_factor', 'interval_days',
            'days_since_last_review',
            
            # â­ TEMPORAL FEATURES (NEW!)
            'last_attempt_1', 'last_attempt_2', 'last_attempt_3',
            'last_attempt_4', 'last_attempt_5',
            'interval_1_2', 'interval_2_3',
            'recent_3_correct', 'total_attempts',
            'learning_trend', 'is_consistent'
        ]
    
    def engineer_advanced_features(self, df):
        """
        Táº¡o thÃªm features tá»« temporal data
        """
        print("ğŸ”§ Engineering advanced temporal features...")
        
        # 1. Streak of correct/wrong
        df['recent_streak'] = df.apply(lambda row: 
            self._calculate_streak([
                row['last_attempt_1'], 
                row['last_attempt_2'], 
                row['last_attempt_3']
            ]), axis=1
        )
        
        # 2. Acceleration (tÄƒng tá»‘c hay cháº­m láº¡i)
        df['learning_acceleration'] = df.apply(lambda row:
            (row['last_attempt_1'] - row['last_attempt_3']) 
            if pd.notna(row['last_attempt_3']) else 0,
            axis=1
        )
        
        # 3. Average interval
        df['avg_interval'] = (df['interval_1_2'] + df['interval_2_3']) / 2
        
        # 4. Interval trend (Ä‘ang tÄƒng hay giáº£m)
        df['interval_trend'] = df['interval_1_2'] - df['interval_2_3']
        
        # 5. Success rate in recent attempts
        df['recent_success_rate'] = df['recent_3_correct'] / 3.0
        
        # Add to feature columns
        self.feature_columns.extend([
            'recent_streak', 'learning_acceleration', 
            'avg_interval', 'interval_trend', 'recent_success_rate'
        ])
        
        print(f"âœ… Total features: {len(self.feature_columns)}")
        
        return df
    
    def _calculate_streak(self, attempts):
        """
        TÃ­nh streak hiá»‡n táº¡i (chuá»—i Ä‘Ãºng hoáº·c sai liÃªn tiáº¿p)
        """
        if pd.isna(attempts[0]):
            return 0
        
        streak = 1
        current = attempts[0]
        
        for i in range(1, len(attempts)):
            if pd.isna(attempts[i]):
                break
            if attempts[i] == current:
                streak += 1
            else:
                break
        
        return streak if current == 1 else -streak
    
    def train(self, df):
        """
        Train XGBoost vá»›i temporal features
        """
        print("\nğŸ¤– Training temporal model...")
        
        # Engineer features
        df = self.engineer_advanced_features(df)
        
        # Prepare data
        X = df[self.feature_columns]
        y = df['forgot']
        
        # Remove null targets
        mask = y.notna()
        X = X[mask]
        y = y[mask]
        
        print(f"Training samples: {len(X)}")
        print(f"Features: {len(self.feature_columns)}")
        print(f"Target distribution:\n{y.value_counts()}")
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Train
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        
        model = xgb.XGBClassifier(
            n_estimators=300,  # More trees for complex patterns
            max_depth=7,       # Deeper for temporal patterns
            learning_rate=0.05, # Lower LR for better generalization
            subsample=0.8,
            colsample_bytree=0.8,
            min_child_weight=3,
            gamma=0.1,
            reg_alpha=0.1,
            reg_lambda=1.0,
            scale_pos_weight=scale_pos_weight,
            objective='binary:logistic',
            eval_metric='auc',
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=30,
            verbose=10
        )
        
        # Evaluate
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        auc = roc_auc_score(y_test, y_pred_proba)
        
        print(f"\nâœ… Training completed!")
        print(f"ğŸ¯ AUC-ROC: {auc:.4f}")
        print(f"\nğŸ“Š Classification Report:")
        print(classification_report(y_test, y_pred, 
                                   target_names=['Remembered', 'Forgot']))
        
        # Feature importance
        self._plot_feature_importance(model)
        
        return model, auc
    
    def _plot_feature_importance(self, model):
        """
        Plot top features
        """
        import matplotlib.pyplot as plt
        
        importance = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ“Š Top 15 Most Important Features:")
        print(feature_importance_df.head(15))
        
        plt.figure(figsize=(12, 8))
        plt.barh(feature_importance_df['feature'][:15], 
                 feature_importance_df['importance'][:15])
        plt.xlabel('Importance')
        plt.title('Top 15 Features - Temporal Model')
        plt.tight_layout()
        plt.savefig('temporal_feature_importance.png')
        print(f"ğŸ’¾ Saved to temporal_feature_importance.png")

# Main
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Temporal Sequence Learning")
    print("=" * 60)
    
    # Fetch data (with temporal features)
    from fetch_training_data import TrainingDataFetcher
    
    API_URL = "http://localhost:8080"
    ADMIN_TOKEN = "your_token"
    
    fetcher = TrainingDataFetcher(API_URL, ADMIN_TOKEN)
    df = fetcher.fetch_training_data()
    
    # Train
    trainer = TemporalModelTrainer()
    model, auc = trainer.train(df)
    
    # Save
    joblib.dump(model, 'models/temporal_smart_review_model.pkl')
    joblib.dump(trainer.feature_columns, 'models/temporal_feature_columns.pkl')
    
    print("\n" + "=" * 60)
    print(f"âœ… Temporal model trained! AUC: {auc:.4f}")
    print("=" * 60)
```


## So SÃ¡nh 3 CÃ¡ch Tiáº¿p Cáº­n

### CÃ¡ch 1: Accuracy-Based (Baseline)

```python
Features: 19 features
- user_total_vocabs, user_accuracy, ...
- times_correct, times_wrong, accuracy_rate
- No temporal information

Expected AUC: 0.75 - 0.80
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n
- âœ… Ãt features

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ KhÃ´ng biáº¿t trend
- âŒ Máº¥t thÃ´ng tin temporal

---

### CÃ¡ch 2: Status-Based

```python
Features: 19 features
Target: status == 'UNKNOWN' â†’ forgot = 1

Expected AUC: 0.72 - 0.78
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Align vá»›i business logic

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Status cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c
- âŒ KhÃ´ng cÃ³ temporal info

---

### CÃ¡ch 3: Temporal Sequence Learning â­ (BEST)

```python
Features: 28 features
- All from CÃ¡ch 1
- + last_attempt_1 to last_attempt_5
- + interval_1_2, interval_2_3
- + learning_trend, recent_streak
- + learning_acceleration
- + interval_trend, recent_success_rate

Expected AUC: 0.82 - 0.88 ğŸ¯
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Há»c tá»« pattern theo thá»i gian
- âœ… Biáº¿t Ä‘Æ°á»£c trend (tiáº¿n bá»™/thoÃ¡i bá»™)
- âœ… PhÃ¡t hiá»‡n consistency
- âœ… Accuracy cao hÆ¡n 5-10%

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Phá»©c táº¡p hÆ¡n
- âŒ Cáº§n nhiá»u data hÆ¡n (min 3-5 attempts/vocab)

---

## Káº¿t Quáº£ Dá»± Kiáº¿n

### Benchmark Comparison

| Metric | Accuracy-Based | Status-Based | **Temporal** |
|--------|---------------|--------------|--------------|
| **AUC-ROC** | 0.78 | 0.75 | **0.85** â­ |
| **Precision** | 0.72 | 0.70 | **0.80** |
| **Recall** | 0.68 | 0.65 | **0.78** |
| **F1-Score** | 0.70 | 0.67 | **0.79** |
| **Training Time** | 2 min | 2 min | 4 min |
| **Features** | 19 | 19 | **28** |

### Feature Importance (Dá»± Ä‘oÃ¡n)

**Top 10 Features trong Temporal Model:**

1. **recent_3_correct** (0.12) - Sá»‘ láº§n Ä‘Ãºng gáº§n nháº¥t
2. **learning_trend** (0.10) - Xu hÆ°á»›ng há»c táº­p
3. **last_attempt_1** (0.09) - Káº¿t quáº£ láº§n cuá»‘i
4. **accuracy_rate** (0.08) - Tá»· lá»‡ Ä‘Ãºng tá»•ng thá»ƒ
5. **recent_streak** (0.07) - Chuá»—i Ä‘Ãºng/sai hiá»‡n táº¡i
6. **days_since_last_review** (0.06)
7. **ef_factor** (0.05)
8. **learning_acceleration** (0.05) - TÄƒng tá»‘c há»c táº­p
9. **interval_1_2** (0.04) - Khoáº£ng cÃ¡ch attempts
10. **vocab_difficulty** (0.04)

**Insight:** Temporal features chiáº¿m **~50% importance**! ğŸ¯


## VÃ­ Dá»¥ Thá»±c Táº¿

### Case Study 1: User Äang Tiáº¿n Bá»™

```json
{
  "word": "sophisticated",
  "user_accuracy": 0.75,
  "times_correct": 6,
  "times_wrong": 4,
  "accuracy_rate": 0.6,
  
  // Temporal features
  "last_attempt_1": 1,  // âœ… ÄÃºng
  "last_attempt_2": 1,  // âœ… ÄÃºng
  "last_attempt_3": 0,  // âŒ Sai
  "last_attempt_4": 0,  // âŒ Sai
  "last_attempt_5": 0,  // âŒ Sai
  
  "recent_3_correct": 2,
  "learning_trend": 1,  // Improving!
  "recent_streak": 2,   // 2 láº§n Ä‘Ãºng liÃªn tiáº¿p
  "learning_acceleration": 1,  // Tá»« sai â†’ Ä‘Ãºng
  
  // Prediction
  "forgot_probability": 0.25  // Tháº¥p! User Ä‘ang há»c tá»‘t
}
```

**Giáº£i thÃ­ch:**
- Máº·c dÃ¹ accuracy_rate chá»‰ 0.6
- NhÆ°ng 2/3 láº§n gáº§n nháº¥t Ä‘Ãºng â†’ **Äang tiáº¿n bá»™**
- Model dá»± Ä‘oÃ¡n: **Kháº£ nÄƒng nhá»› cao** âœ…

---

### Case Study 2: User Äang QuÃªn

```json
{
  "word": "procrastinate",
  "user_accuracy": 0.75,
  "times_correct": 6,
  "times_wrong": 4,
  "accuracy_rate": 0.6,
  
  // Temporal features
  "last_attempt_1": 0,  // âŒ Sai
  "last_attempt_2": 0,  // âŒ Sai
  "last_attempt_3": 1,  // âœ… ÄÃºng
  "last_attempt_4": 1,  // âœ… ÄÃºng
  "last_attempt_5": 1,  // âœ… ÄÃºng
  
  "recent_3_correct": 1,
  "learning_trend": -1,  // Declining!
  "recent_streak": -2,   // 2 láº§n sai liÃªn tiáº¿p
  "learning_acceleration": -1,  // Tá»« Ä‘Ãºng â†’ sai
  
  // Prediction
  "forgot_probability": 0.85  // Cao! User Ä‘ang quÃªn
}
```

**Giáº£i thÃ­ch:**
- CÃ¹ng accuracy_rate 0.6 nhÆ° Case 1
- NhÆ°ng 2/3 láº§n gáº§n nháº¥t sai â†’ **Äang thoÃ¡i bá»™**
- Model dá»± Ä‘oÃ¡n: **Kháº£ nÄƒng quÃªn cao** âŒ

**â†’ Snapshot-based khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c 2 cases nÃ y!**
**â†’ Temporal model phÃ¢n biá»‡t chÃ­nh xÃ¡c!** âœ…

