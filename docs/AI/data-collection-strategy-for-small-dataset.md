# Chi·∫øn L∆∞·ª£c Thu Th·∫≠p D·ªØ Li·ªáu Cho B√†i To√°n V·ªõi Dataset Nh·ªè

## V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

**T√¨nh hu·ªëng:**
- D·ªØ li·ªáu ch·ªâ c√≥ t·ª´ b·∫£ng `user_vocab_progress`
- S·ªë l∆∞·ª£ng records √≠t (< 10K samples)
- Ch∆∞a ƒë·ªß ƒë·ªÉ train model ML hi·ªáu qu·∫£

**Y√™u c·∫ßu t·ªëi thi·ªÉu:**
- **Baseline model**: 1,000 - 5,000 samples
- **Good model**: 10,000 - 50,000 samples
- **Excellent model**: > 50,000 samples

---

## Gi·∫£i Ph√°p 1: Thu Th·∫≠p D·ªØ Li·ªáu T·ª´ Nhi·ªÅu Ngu·ªìn

### 1.1 Khai Th√°c D·ªØ Li·ªáu T·ª´ Game Sessions

```sql
-- L·∫•y d·ªØ li·ªáu t·ª´ game sessions (QuickQuiz, ImageWordMatching, etc.)
SELECT 
    gsd.user_id,
    gsd.vocab_id,
    gsd.is_correct,
    gsd.time_taken,
    gsd.created_at,
    v.cefr,
    v.word,
    LENGTH(v.word) as vocab_length,
    gs.game_id
FROM game_session_details gsd
JOIN game_sessions gs ON gsd.session_id = gs.id
JOIN vocabs v ON gsd.vocab_id = v.id
WHERE gsd.created_at >= NOW() - INTERVAL '6 months'
ORDER BY gsd.created_at DESC;
```

**L·ª£i √≠ch:**
- C√≥ nhi·ªÅu data h∆°n t·ª´ game sessions
- M·ªói l·∫ßn ch∆°i game = 1 data point
- C√≥ th·ªÉ c√≥ h√†ng ch·ª•c ngh√¨n records

### 1.2 T·∫°o Synthetic Training Data

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def create_synthetic_data(existing_data, n_samples=10000):
    """
    T·∫°o synthetic data d·ª±a tr√™n distribution c·ªßa data th·∫≠t
    """
    synthetic_data = []
    
    for _ in range(n_samples):
        # Sample t·ª´ distribution th·∫≠t
        base_sample = existing_data.sample(1).iloc[0]
        
        # Add noise
        synthetic_sample = {
            'user_total_vocabs': int(base_sample['user_total_vocabs'] * np.random.uniform(0.8, 1.2)),
            'user_accuracy': np.clip(base_sample['user_accuracy'] + np.random.normal(0, 0.1), 0, 1),
            'vocab_difficulty': base_sample['vocab_difficulty'],
            'vocab_length': base_sample['vocab_length'],
            'times_correct': int(base_sample['times_correct'] * np.random.uniform(0.5, 1.5)),
            'times_wrong': int(base_sample['times_wrong'] * np.random.uniform(0.5, 1.5)),
            'repetition': base_sample['repetition'] + np.random.randint(-1, 2),
            'ef_factor': np.clip(base_sample['ef_factor'] + np.random.normal(0, 0.2), 1.3, 3.0),
            'interval_days': max(1, int(base_sample['interval_days'] * np.random.uniform(0.7, 1.3))),
            'days_since_last_review': max(0, int(base_sample['days_since_last_review'] + np.random.randint(-3, 3))),
            'days_until_next_review': int(base_sample['days_until_next_review'] + np.random.randint(-2, 2)),
            'forgot': base_sample['forgot']  # Keep target
        }
        
        synthetic_data.append(synthetic_sample)
    
    return pd.DataFrame(synthetic_data)

# Usage
synthetic_df = create_synthetic_data(real_data, n_samples=10000)
combined_df = pd.concat([real_data, synthetic_df])
```


### 1.3 K·∫øt H·ª£p Nhi·ªÅu B·∫£ng

```sql
-- Query t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn
WITH user_stats AS (
    SELECT 
        user_id,
        COUNT(DISTINCT vocab_id) as total_vocabs,
        AVG(CASE WHEN times_correct + times_wrong > 0 
            THEN times_correct::float / (times_correct + times_wrong) 
            ELSE 0 END) as user_accuracy
    FROM user_vocab_progress
    GROUP BY user_id
),
game_results AS (
    SELECT 
        gsd.user_id,
        gsd.vocab_id,
        gsd.is_correct,
        gsd.time_taken,
        gsd.created_at,
        v.cefr,
        LENGTH(v.word) as vocab_length
    FROM game_session_details gsd
    JOIN vocabs v ON gsd.vocab_id = v.id
),
vocab_progress AS (
    SELECT 
        uvp.user_id,
        uvp.vocab_id,
        uvp.times_correct,
        uvp.times_wrong,
        uvp.repetition,
        uvp.ef_factor,
        uvp.interval_days,
        uvp.last_reviewed,
        uvp.next_review_date,
        uvp.status
    FROM user_vocab_progress uvp
)
SELECT 
    us.user_id,
    gr.vocab_id,
    us.total_vocabs as user_total_vocabs,
    us.user_accuracy,
    CASE gr.cefr 
        WHEN 'A1' THEN 1 WHEN 'A2' THEN 2 
        WHEN 'B1' THEN 3 WHEN 'B2' THEN 4 
        WHEN 'C1' THEN 5 WHEN 'C2' THEN 6 
        ELSE 3 END as vocab_difficulty,
    gr.vocab_length,
    COALESCE(vp.times_correct, 0) as times_correct,
    COALESCE(vp.times_wrong, 0) as times_wrong,
    COALESCE(vp.repetition, 0) as repetition,
    COALESCE(vp.ef_factor, 2.5) as ef_factor,
    COALESCE(vp.interval_days, 1) as interval_days,
    COALESCE(EXTRACT(DAY FROM (CURRENT_DATE - vp.last_reviewed)), 999) as days_since_last_review,
    COALESCE(EXTRACT(DAY FROM (vp.next_review_date - CURRENT_DATE)), 0) as days_until_next_review,
    -- Target: D·ª± ƒëo√°n t·ª´ game result ti·∫øp theo
    CASE WHEN gr.is_correct = false THEN 1 ELSE 0 END as forgot
FROM user_stats us
JOIN game_results gr ON us.user_id = gr.user_id
LEFT JOIN vocab_progress vp ON gr.user_id = vp.user_id AND gr.vocab_id = vp.vocab_id
WHERE gr.created_at >= NOW() - INTERVAL '6 months'
ORDER BY gr.created_at DESC;
```

**∆Ø·ªõc t√≠nh s·ªë l∆∞·ª£ng data:**
- N·∫øu c√≥ 100 users
- M·ªói user ch∆°i 50 games
- M·ªói game c√≥ 10 questions
- ‚Üí **50,000 data points** üéâ


## Gi·∫£i Ph√°p 2: Rule-Based Model Tr∆∞·ªõc, ML Model Sau

### Phase 1: Rule-Based Recommendation (Ngay l·∫≠p t·ª©c)

Kh√¥ng c·∫ßn training data, ch·ªâ c·∫ßn logic:

```python
def rule_based_recommendation(user_progress_list):
    """
    G·ª£i √Ω d·ª±a tr√™n rules ƒë∆°n gi·∫£n
    Kh√¥ng c·∫ßn ML model
    """
    recommendations = []
    
    for progress in user_progress_list:
        priority_score = 0
        reasons = []
        
        # Rule 1: Overdue (40 points)
        if progress.days_until_next_review < 0:
            overdue_days = abs(progress.days_until_next_review)
            priority_score += min(overdue_days * 4, 40)
            reasons.append(f"Qu√° h·∫°n {overdue_days} ng√†y")
        
        # Rule 2: Low accuracy (30 points)
        total_attempts = progress.times_correct + progress.times_wrong
        if total_attempts > 0:
            accuracy = progress.times_correct / total_attempts
            if accuracy < 0.5:
                priority_score += 30
                reasons.append("T·ª∑ l·ªá ƒë√∫ng th·∫•p")
            elif accuracy < 0.7:
                priority_score += 15
        
        # Rule 3: Difficult vocab (20 points)
        if progress.vocab.cefr in ['C1', 'C2']:
            priority_score += 20
            reasons.append("T·ª´ kh√≥")
        elif progress.vocab.cefr in ['B2']:
            priority_score += 10
        
        # Rule 4: Due today (10 points)
        if progress.days_until_next_review == 0:
            priority_score += 10
            reasons.append("ƒê·∫øn h·∫°n h√¥m nay")
        
        recommendations.append({
            'vocab_id': progress.vocab.id,
            'priority_score': priority_score,
            'reasons': ' ‚Ä¢ '.join(reasons)
        })
    
    # Sort by priority
    recommendations.sort(key=lambda x: x['priority_score'], reverse=True)
    
    return recommendations
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Kh√¥ng c·∫ßn training data
- ‚úÖ Deploy ngay l·∫≠p t·ª©c
- ‚úÖ D·ªÖ hi·ªÉu, d·ªÖ explain
- ‚úÖ C√≥ th·ªÉ tune rules d·ª±a tr√™n feedback

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Kh√¥ng personalized
- ‚ùå Kh√¥ng h·ªçc t·ª´ data
- ‚ùå Accuracy th·∫•p h∆°n ML


### Phase 2: Hybrid Model (Sau 1-2 th√°ng)

K·∫øt h·ª£p rule-based v√† ML:

```python
class HybridRecommendationSystem:
    def __init__(self, ml_model=None, min_samples_for_ml=1000):
        self.ml_model = ml_model
        self.min_samples_for_ml = min_samples_for_ml
        self.use_ml = ml_model is not None
    
    def recommend(self, user_progress_list):
        """
        Hybrid approach:
        - N·∫øu c√≥ ƒë·ªß data ‚Üí d√πng ML
        - N·∫øu kh√¥ng ƒë·ªß data ‚Üí d√πng rules
        - K·∫øt h·ª£p c·∫£ 2 v·ªõi weights
        """
        # Rule-based scores
        rule_scores = self._get_rule_based_scores(user_progress_list)
        
        if self.use_ml and len(user_progress_list) >= self.min_samples_for_ml:
            # ML-based scores
            ml_scores = self._get_ml_scores(user_progress_list)
            
            # Combine: 60% ML + 40% Rules
            final_scores = []
            for rule_score, ml_score in zip(rule_scores, ml_scores):
                combined_score = 0.6 * ml_score['score'] + 0.4 * rule_score['score']
                final_scores.append({
                    'vocab_id': rule_score['vocab_id'],
                    'score': combined_score,
                    'ml_score': ml_score['score'],
                    'rule_score': rule_score['score'],
                    'reason': rule_score['reason']
                })
        else:
            # Ch·ªâ d√πng rules
            final_scores = rule_scores
        
        # Sort by score
        final_scores.sort(key=lambda x: x['score'], reverse=True)
        
        return final_scores
    
    def _get_rule_based_scores(self, user_progress_list):
        # Implementation t·ª´ phase 1
        pass
    
    def _get_ml_scores(self, user_progress_list):
        # Predict using ML model
        features = self._extract_features(user_progress_list)
        predictions = self.ml_model.predict_proba(features)[:, 1]
        
        return [
            {'vocab_id': p.vocab.id, 'score': pred}
            for p, pred in zip(user_progress_list, predictions)
        ]
```

### Phase 3: Pure ML Model (Sau 3-6 th√°ng)

Khi ƒë√£ c√≥ ƒë·ªß data (> 10K samples):

```python
# Train XGBoost v·ªõi data th·∫≠t
model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    # ... other params
)

model.fit(X_train, y_train)

# Deploy v√† thay th·∫ø rule-based ho√†n to√†n
```


## Gi·∫£i Ph√°p 3: Transfer Learning & Pre-trained Models

### 3.1 S·ª≠ d·ª•ng SM-2 Algorithm Nh∆∞ Features

```python
def calculate_sm2_forgot_probability(progress):
    """
    S·ª≠ d·ª•ng SM-2 algorithm ƒë·ªÉ t√≠nh x√°c su·∫•t qu√™n
    Kh√¥ng c·∫ßn training data!
    """
    # SM-2 factors
    ef_factor = progress.ef_factor
    interval_days = progress.interval_days
    days_since_last_review = progress.days_since_last_review
    days_until_next_review = progress.days_until_next_review
    
    # Calculate forgetting curve (Ebbinghaus)
    # R = e^(-t/S)
    # R: retention, t: time, S: strength (related to EF factor)
    
    strength = ef_factor * interval_days
    time_elapsed = days_since_last_review
    
    retention = np.exp(-time_elapsed / strength)
    forgot_probability = 1 - retention
    
    return forgot_probability

# S·ª≠ d·ª•ng nh∆∞ baseline
for progress in user_progress_list:
    progress.forgot_prob_sm2 = calculate_sm2_forgot_probability(progress)
```

### 3.2 Pretrain Tr√™n Public Dataset

```python
# S·ª≠ d·ª•ng public spaced repetition dataset
# V√≠ d·ª•: Anki dataset, Duolingo dataset

# 1. Download public dataset
import requests
public_data = pd.read_csv('https://example.com/spaced_repetition_data.csv')

# 2. Map features to your schema
public_data_mapped = map_features(public_data)

# 3. Pretrain model
pretrained_model = xgb.XGBClassifier()
pretrained_model.fit(public_data_mapped[features], public_data_mapped['forgot'])

# 4. Fine-tune v·ªõi data c·ªßa b·∫°n (khi c√≥ ƒë·ªß)
pretrained_model.fit(
    your_data[features], 
    your_data['forgot'],
    xgb_model=pretrained_model.get_booster()  # Continue training
)
```


## Gi·∫£i Ph√°p 4: Active Learning - Thu Th·∫≠p Data Th√¥ng Minh

### 4.1 Chi·∫øn L∆∞·ª£c Thu Th·∫≠p

```python
class ActiveLearningDataCollector:
    """
    Thu th·∫≠p data m·ªôt c√°ch th√¥ng minh
    ∆Øu ti√™n nh·ªØng cases kh√≥ predict
    """
    
    def __init__(self, model, uncertainty_threshold=0.4):
        self.model = model
        self.uncertainty_threshold = uncertainty_threshold
        self.collected_samples = []
    
    def should_collect_feedback(self, features):
        """
        Quy·∫øt ƒë·ªãnh c√≥ n√™n h·ªèi user feedback kh√¥ng
        """
        if self.model is None:
            return True  # Lu√¥n collect n·∫øu ch∆∞a c√≥ model
        
        # Predict
        proba = self.model.predict_proba([features])[0]
        confidence = max(proba)
        
        # N·∫øu model kh√¥ng ch·∫Øc ch·∫Øn ‚Üí collect feedback
        if confidence < (0.5 + self.uncertainty_threshold):
            return True
        
        return False
    
    def collect_feedback(self, vocab_id, user_id, features, actual_result):
        """
        L∆∞u feedback t·ª´ user
        """
        sample = {
            'vocab_id': vocab_id,
            'user_id': user_id,
            'features': features,
            'forgot': actual_result,  # 1 if forgot, 0 if remembered
            'collected_at': datetime.now()
        }
        
        self.collected_samples.append(sample)
        
        # Retrain model khi c√≥ ƒë·ªß samples m·ªõi
        if len(self.collected_samples) >= 100:
            self.retrain_model()
    
    def retrain_model(self):
        """
        Retrain model v·ªõi data m·ªõi
        """
        new_data = pd.DataFrame(self.collected_samples)
        # Combine v·ªõi data c≈© v√† retrain
        # ...
        self.collected_samples = []  # Reset
```

### 4.2 UI Flow ƒê·ªÉ Thu Th·∫≠p Feedback

```typescript
// Frontend: Sau khi user √¥n t·∫≠p
interface ReviewFeedback {
  vocabId: string;
  remembered: boolean;  // User c√≥ nh·ªõ kh√¥ng?
  confidence: 'low' | 'medium' | 'high';  // ƒê·ªô t·ª± tin
  timeSpent: number;  // Th·ªùi gian suy nghƒ©
}

async function submitReviewFeedback(feedback: ReviewFeedback) {
  // G·ª≠i feedback v·ªÅ backend
  await api.post('/api/v1/smart-review/feedback', feedback);
  
  // Backend s·∫Ω l∆∞u v√†o training data
}
```

```java
// Backend: L∆∞u feedback
@PostMapping("/feedback")
public ResponseEntity<ApiResponse<Void>> submitFeedback(
    @AuthenticationPrincipal User user,
    @RequestBody ReviewFeedback feedback) {
    
    // L∆∞u v√†o b·∫£ng training_data
    TrainingData data = TrainingData.builder()
        .userId(user.getId())
        .vocabId(feedback.getVocabId())
        .remembered(feedback.getRemembered())
        .confidence(feedback.getConfidence())
        .timeSpent(feedback.getTimeSpent())
        .collectedAt(LocalDateTime.now())
        .build();
    
    trainingDataRepository.save(data);
    
    return ResponseEntity.ok(ApiResponse.success("Feedback saved"));
}
```


## Gi·∫£i Ph√°p 5: Cold Start Problem - X·ª≠ L√Ω User/Vocab M·ªõi

### 5.1 User M·ªõi (Ch∆∞a C√≥ L·ªãch S·ª≠)

```python
def get_recommendations_for_new_user(user_id, vocab_list):
    """
    G·ª£i √Ω cho user m·ªõi d·ª±a tr√™n:
    - ƒê·ªô kh√≥ c·ªßa t·ª´ (CEFR)
    - T·∫ßn su·∫•t xu·∫•t hi·ªán
    - Popularity (t·ª´ ph·ªï bi·∫øn)
    """
    recommendations = []
    
    for vocab in vocab_list:
        # Default priority cho user m·ªõi
        priority = 50  # Base score
        
        # ∆Øu ti√™n t·ª´ d·ªÖ tr∆∞·ªõc (A1, A2)
        if vocab.cefr == 'A1':
            priority += 20
        elif vocab.cefr == 'A2':
            priority += 15
        elif vocab.cefr == 'B1':
            priority += 10
        
        # ∆Øu ti√™n t·ª´ ph·ªï bi·∫øn
        if vocab.frequency_rank < 1000:  # Top 1000 words
            priority += 15
        elif vocab.frequency_rank < 3000:
            priority += 10
        
        recommendations.append({
            'vocab_id': vocab.id,
            'priority': priority,
            'reason': 'T·ª´ c∆° b·∫£n, ph·ªï bi·∫øn'
        })
    
    recommendations.sort(key=lambda x: x['priority'], reverse=True)
    return recommendations
```

### 5.2 Vocab M·ªõi (Ch∆∞a C√≥ Ai H·ªçc)

```python
def estimate_difficulty_for_new_vocab(vocab):
    """
    ∆Ø·ªõc t√≠nh ƒë·ªô kh√≥ c·ªßa t·ª´ m·ªõi d·ª±a tr√™n:
    - CEFR level
    - ƒê·ªô d√†i t·ª´
    - S·ªë √¢m ti·∫øt
    - C√≥ ph·∫£i t·ª´ gh√©p kh√¥ng
    """
    difficulty_score = 0
    
    # CEFR level
    cefr_scores = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}
    difficulty_score += cefr_scores.get(vocab.cefr, 3) * 10
    
    # ƒê·ªô d√†i t·ª´
    word_length = len(vocab.word)
    if word_length > 10:
        difficulty_score += 20
    elif word_length > 7:
        difficulty_score += 10
    
    # S·ªë √¢m ti·∫øt (estimate)
    syllables = estimate_syllables(vocab.word)
    difficulty_score += syllables * 3
    
    # T·ª´ gh√©p (c√≥ d·∫•u g·∫°ch ngang)
    if '-' in vocab.word:
        difficulty_score += 5
    
    return difficulty_score

def estimate_syllables(word):
    """Simple syllable counter"""
    vowels = 'aeiou'
    count = 0
    prev_was_vowel = False
    
    for char in word.lower():
        is_vowel = char in vowels
        if is_vowel and not prev_was_vowel:
            count += 1
        prev_was_vowel = is_vowel
    
    return max(1, count)
```

